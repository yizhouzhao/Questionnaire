{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acronym_identification', 'ade_corpus_v2', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews', 'allocine', 'alt', 'amazon_polarity', 'amazon_reviews_multi', 'amazon_us_reviews', 'ambig_qa', 'amttl', 'anli', 'app_reviews', 'aqua_rat', 'aquamuse', 'ar_cov19', 'ar_res_reviews', 'arabic_billion_words', 'arabic_pos_dialect', 'arcd', 'arsentd_lev', 'art', 'arxiv_dataset', 'aslg_pc12', 'asnq', 'asset', 'assin', 'assin2', 'atomic', 'autshumato', 'bc2gm_corpus', 'best2009', 'bianet', 'bible_para', 'big_patent', 'billsum', 'bing_coronavirus_query_set', 'biomrc', 'blended_skill_talk', 'blimp', 'blog_authorship_corpus', 'bookcorpus', 'bookcorpusopen', 'boolq', 'bprec', 'break_data', 'brwac', 'bsd_ja_en', 'bswac', 'c3', 'c4', 'cail2018', 'capes', 'catalonia_independence', 'cawac', 'cc100', 'cdsc', 'cdt', 'cfq', 'chr_en', 'cifar10', 'circa', 'civil_comments', 'clickbait_news_bg', 'climate_fever', 'clinc_oos', 'clue', 'cmrc2018', 'cnn_dailymail', 'coached_conv_pref', 'coarse_discourse', 'codah', 'code_search_net', 'com_qa', 'common_gen', 'commonsense_qa', 'compguesswhat', 'conceptnet5', 'conll2000', 'conll2002', 'conll2003', 'conv_ai', 'conv_ai_2', 'conv_ai_3', 'coqa', 'cornell_movie_dialog', 'cos_e', 'cosmos_qa', 'counter', 'covid_qa_castorini', 'covid_qa_deepset', 'covid_qa_ucsd', 'covid_tweets_japanese', 'craigslist_bargains', 'crawl_domain', 'crd3', 'crime_and_punish', 'crows_pairs', 'cs_restaurants', 'csv', 'curiosity_dialogs', 'daily_dialog', 'dane', 'danish_political_comments', 'dart', 'datacommons_factcheck', 'dbpedia_14', 'dbrd', 'deal_or_no_dialog', 'definite_pronoun_resolution', 'dengue_filipino', 'dialog_re', 'diplomacy_detection', 'disaster_response_messages', 'discofuse', 'discovery', 'doc2dial', 'docred', 'doqa', 'dream', 'drop', 'dutch_social', 'dyk', 'e2e_nlg', 'e2e_nlg_cleaned', 'ecb', 'ehealth_kd', 'eitb_parcc', 'eli5', 'emea', 'emo', 'emotion', 'emotone_ar', 'empathetic_dialogues', 'enriched_web_nlg', 'eraser_multi_rc', 'esnli', 'eth_py150_open', 'ethos', 'euronews', 'europa_eac_tm', 'europa_ecdc_tm', 'event2Mind', 'evidence_infer_treatment', 'exams', 'factckbr', 'fake_news_english', 'fake_news_filipino', 'farsi_news', 'fever', 'finer', 'flores', 'flue', 'fquad', 'gap', 'generated_reviews_enth', 'generics_kb', 'german_legal_entity_recognition', 'germaner', 'germeval_14', 'giga_fren', 'gigaword', 'glucose', 'glue', 'gnad10', 'go_emotions', 'google_wellformed_query', 'grail_qa', 'great_code', 'guardian_authorship', 'gutenberg_time', 'hans', 'hansards', 'hard', 'harem', 'has_part', 'hate_offensive', 'hate_speech18', 'hate_speech_filipino', 'hate_speech_offensive', 'hate_speech_pl', 'hate_speech_portuguese', 'hausa_voa_ner', 'hausa_voa_topics', 'head_qa', 'health_fact', 'hebrew_projectbenyehuda', 'hebrew_sentiment', 'hebrew_this_world', 'hellaswag', 'hind_encorp', 'hindi_discourse', 'hippocorpus', 'hkcancor', 'hope_edi', 'hotpot_qa', 'hover', 'hrenwac_para', 'hrwac', 'humicroedit', 'hybrid_qa', 'hyperpartisan_news_detection', 'id_clickbait', 'id_nergrit_corpus', 'id_newspapers_2018', 'id_panl_bppt', 'id_puisi', 'igbo_english_machine_translation', 'igbo_monolingual', 'igbo_ner', 'ilist', 'imdb', 'imdb_urdu_reviews', 'imppres', 'indic_glue', 'indonlu', 'inquisitive_qg', 'interpress_news_category_tr', 'isixhosa_ner_corpus', 'isizulu_ner_corpus', 'iwslt2017', 'jeopardy', 'jfleg', 'jigsaw_toxicity_pred', 'jnlpba', 'journalists_questions', 'json', 'kannada_news', 'kd_conv', 'kde4', 'kelm', 'kilt_tasks', 'kilt_wikipedia', 'kinnews_kirnews', 'kor_hate', 'kor_ner', 'kor_nli', 'kor_nlu', 'kor_qpair', 'kor_sarcasm', 'labr', 'lama', 'lambada', 'large_spanish_corpus', 'lc_quad', 'lener_br', 'liar', 'librispeech_lm', 'limit', 'lince', 'linnaeus', 'liveqa', 'lm1b', 'lst20', 'mac_morpho', 'makhzan', 'math_dataset', 'math_qa', 'matinf', 'mc_taco', 'md_gender_bias', 'med_hop', 'medal', 'medical_dialog', 'medical_questions_pairs', 'menyo20k_mt', 'meta_woz', 'metooma', 'metrec', 'mkb', 'mkqa', 'mlqa', 'mlsum', 'mocha', 'movie_rationales', 'mrqa', 'ms_marco', 'ms_terms', 'msr_genomics_kbcomp', 'msr_sqa', 'msr_text_compression', 'msr_zhen_translation_parity', 'msra_ner', 'mt_eng_vietnamese', 'muchocine', 'multi_booked', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'multi_para_crawl', 'multi_re_qa', 'multi_woz_v22', 'multi_x_science_sum', 'mutual_friends', 'mwsc', 'myanmar_news', 'narrativeqa', 'natural_questions', 'ncbi_disease', 'nchlt', 'ncslgr', 'nell', 'neural_code_search', 'news_commentary', 'newsgroup', 'newsph', 'newsph_nli', 'newsqa', 'newsroom', 'nkjp-ner', 'nli_tr', 'norwegian_ner', 'nq_open', 'nsmc', 'numer_sense', 'numeric_fused_head', 'oclar', 'offcombr', 'offenseval2020_tr', 'offenseval_dravidian', 'ofis_publik', 'ohsumed', 'omp', 'onestop_english', 'open_subtitles', 'openbookqa', 'openwebtext', 'opinosis', 'opus100', 'opus_books', 'opus_dgt', 'opus_dogc', 'opus_elhuyar', 'opus_euconst', 'opus_finlex', 'opus_fiskmo', 'opus_gnome', 'opus_infopankki', 'opus_memat', 'opus_montenegrinsubs', 'opus_openoffice', 'opus_paracrawl', 'opus_rf', 'opus_tedtalks', 'opus_ubuntu', 'opus_wikipedia', 'opus_xhosanavy', 'orange_sum', 'pandas', 'para_crawl', 'para_pat', 'paws', 'paws-x', 'pec', 'peer_read', 'peoples_daily_ner', 'per_sent', 'persian_ner', 'pg19', 'php', 'piaf', 'pib', 'piqa', 'poem_sentiment', 'polemo2', 'poleval2019_cyberbullying', 'poleval2019_mt', 'polsum', 'polyglot_ner', 'prachathai67k', 'pragmeval', 'proto_qa', 'psc', 'ptb_text_only', 'pubmed', 'pubmed_qa', 'py_ast', 'qa4mre', 'qa_srl', 'qa_zre', 'qangaroo', 'qanta', 'qasc', 'qed', 'qed_amara', 'quac', 'quail', 'quarel', 'quartz', 'quora', 'quoref', 'race', 're_dial', 'reasoning_bg', 'recipe_nlg', 'reclor', 'reddit', 'reddit_tifu', 'refresd', 'reuters21578', 'roman_urdu', 'ronec', 'ropes', 'rotten_tomatoes', 'samsum', 'sanskrit_classic', 'saudinewsnet', 'scan', 'scb_mt_enth_2020', 'schema_guided_dstc8', 'scicite', 'scielo', 'scientific_papers', 'scifact', 'sciq', 'scitail', 'scitldr', 'search_qa', 'selqa', 'sem_eval_2010_task_8', 'sem_eval_2014_task_1', 'sem_eval_2020_task_11', 'sent_comp', 'senti_lex', 'senti_ws', 'sentiment140', 'sepedi_ner', 'sesotho_ner_corpus', 'setimes', 'setswana_ner_corpus', 'sharc', 'sharc_modified', 'simple_questions_v2', 'siswati_ner_corpus', 'smartdata', 'sms_spam', 'snips_built_in_intents', 'snli', 'snow_simplified_japanese_corpus', 'so_stacksample', 'social_bias_frames', 'social_i_qa', 'sofc_materials_articles', 'sogou_news', 'spanish_billion_words', 'spc', 'species_800', 'spider', 'squad', 'squad_adversarial', 'squad_es', 'squad_it', 'squad_kor_v1', 'squad_kor_v2', 'squad_v1_pt', 'squad_v2', 'squadshifts', 'srwac', 'stereoset', 'stsb_mt_sv', 'style_change_detection', 'super_glue', 'swag', 'swahili', 'swahili_news', 'swedish_ner_corpus', 'swedish_reviews', 'tab_fact', 'tamilmixsentiment', 'tanzil', 'tapaco', 'tashkeela', 'taskmaster1', 'taskmaster2', 'taskmaster3', 'tatoeba', 'ted_hrlr', 'ted_iwlst2013', 'ted_multi', 'telugu_books', 'telugu_news', 'tep_en_fa_para', 'text', 'thai_toxicity_tweet', 'thainer', 'thaiqa_squad', 'thaisum', 'tilde_model', 'times_of_india_news_headlines', 'tiny_shakespeare', 'tlc', 'tmu_gfm_dataset', 'totto', 'trec', 'trivia_qa', 'tsac', 'ttc4900', 'tunizi', 'tuple_ie', 'turkish_movie_sentiment', 'turkish_ner', 'turkish_shrinked_ner', 'turku_ner_corpus', 'tweet_qa', 'tweets_ar_en_parallel', 'tweets_hate_speech_detection', 'twi_text_c3', 'twi_wordsim353', 'tydiqa', 'ubuntu_dialogs_corpus', 'udhr', 'um005', 'un_ga', 'un_multi', 'un_pc', 'universal_dependencies', 'urdu_fake_news', 'urdu_sentiment_corpus', 'web_nlg', 'web_of_science', 'web_questions', 'weibo_ner', 'wi_locness', 'wiki40b', 'wiki_asp', 'wiki_atomic_edits', 'wiki_auto', 'wiki_bio', 'wiki_dpr', 'wiki_hop', 'wiki_lingua', 'wiki_movies', 'wiki_qa', 'wiki_qa_ar', 'wiki_snippets', 'wiki_source', 'wiki_split', 'wiki_summary', 'wikiann', 'wikicorpus', 'wikihow', 'wikipedia', 'wikisql', 'wikitext', 'wikitext_tl39', 'wili_2018', 'wino_bias', 'winograd_wsc', 'winogrande', 'wiqa', 'wisesight1000', 'wisesight_sentiment', 'wmt14', 'wmt15', 'wmt16', 'wmt17', 'wmt18', 'wmt19', 'wmt20_mlqe_task1', 'wmt20_mlqe_task2', 'wmt20_mlqe_task3', 'wmt_t2t', 'wnut_17', 'wongnai_reviews', 'woz_dialogue', 'wrbsc', 'x_stance', 'xcopa', 'xed_en_fi', 'xglue', 'xnli', 'xor_tydi_qa', 'xquad', 'xquad_r', 'xsum', 'xsum_factuality', 'xtreme', 'yahoo_answers_qa', 'yahoo_answers_topics', 'yelp_polarity', 'yelp_review_full', 'yoruba_bbc_topics', 'yoruba_gv_ner', 'yoruba_text_c3', 'yoruba_wordsim353', 'youtube_caption_corrections', 'zest', 'Fraser/news-category-dataset', 'Fraser/python-lines', 'Telecom-Paris/SILICONE', 'cdminix/mgb1', 'eusip/SILICONE', 'german-nlp-group/german_common_crawl', 'joelito/ler', 'joelito/sem_eval_2010_task_8', 'k-halid/ar', 'lhoestq/squad', 'mulcyber/europarl-mono', 'piEsposito/br-quad-2.0', 'piEsposito/br_quad_20', 'piEsposito/squad_20_ptbr', 'sshleifer/pseudo_bart_xsum']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import list_datasets, load_dataset\n",
    "\n",
    "datasets_list = list_datasets()\n",
    "\n",
    "print(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMDB Yelp-2 Yelp-5 DBpedia AG Amazon-2 Amazon-5\n",
    "# dataset_name_list = [\"imdb\", \"dbpedia_14\", \"ag_news\", \"amazon_polarity\", \"yelp_polarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (C:\\Users\\Yizhou Zhao\\.cache\\huggingface\\datasets\\rotten_tomatoes_movie_review\\default\\1.0.0\\9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a)\n"
     ]
    }
   ],
   "source": [
    "other_dataset_name_list = [\"rotten_tomatoes\"]\n",
    "\n",
    "dataset = load_dataset('rotten_tomatoes', split='train')\n",
    "\n",
    "# %load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': 'the film is exhilarating to watch because sandler , liberated from the constraints of formula , reveals unexpected depths as an actor .'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[np.random.randint(1000)])\n",
    "\n",
    "set([item[\"label\"] for item in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from design.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QAM = QAMachine(\"design/questions.csv\", \"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QAM.question_collection.question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(QAM.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "QAM.conduct_survey([j for j in range(len(QAM.question_collection))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = generate_unifiedqa_text(QAM.question_collection.question_list[question_id], \n",
    "#                             QAM.question_collection.answer_list[question_id], \n",
    "#                             QAM.dataset[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = []\n",
    "for question_id in [j for j in range(10)]:\n",
    "    text = generate_unifiedqa_text(QAM.question_collection.question_list[question_id], \n",
    "                QAM.question_collection.answer_list[question_id], \n",
    "                QAM.dataset[i]['text'])\n",
    "    batch_sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = QAM.tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = QAM.model.generate(input_ids.to())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[QAM.tokenizer.decode(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QAM.survey[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
